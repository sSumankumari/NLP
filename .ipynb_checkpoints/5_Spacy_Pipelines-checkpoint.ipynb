{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5975426",
   "metadata": {},
   "source": [
    "# Spacy Language Processing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c09ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e1943d",
   "metadata": {},
   "source": [
    "### Blank nlp pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc2dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Captain America ate 100$ of samosa. Then he said I can do this all day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6fbd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      "America\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "I\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806e8d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |    |  \n",
      "America  |    |  \n",
      "ate  |    |  \n",
      "100  |    |  \n",
      "$  |    |  \n",
      "of  |    |  \n",
      "samosa  |    |  \n",
      ".  |    |  \n",
      "Then  |    |  \n",
      "he  |    |  \n",
      "said  |    |  \n",
      "I  |    |  \n",
      "can  |    |  \n",
      "do  |    |  \n",
      "this  |    |  \n",
      "all  |    |  \n",
      "day  |    |  \n",
      ".  |    |  \n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5932d3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b96251",
   "metadata": {},
   "source": [
    "### Using Trained Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7badf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906bba5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6f3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24c48df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1c41d17b940>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1c41d17b520>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1c41d165ee0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1c41d46a2c0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1c41d2f7d80>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1c41d1654d0>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "119a4b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |  PROPN  |  Captain\n",
      "America  |  PROPN  |  America\n",
      "ate  |  VERB  |  eat\n",
      "100  |  NUM  |  100\n",
      "$  |  NUM  |  $\n",
      "of  |  ADP  |  of\n",
      "samosa  |  PROPN  |  samosa\n",
      ".  |  PUNCT  |  .\n",
      "Then  |  ADV  |  then\n",
      "he  |  PRON  |  he\n",
      "said  |  VERB  |  say\n",
      "I  |  PRON  |  I\n",
      "can  |  AUX  |  can\n",
      "do  |  VERB  |  do\n",
      "this  |  PRON  |  this\n",
      "all  |  DET  |  all\n",
      "day  |  NOUN  |  day\n",
      ".  |  PUNCT  |  .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Captain America ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3348b167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430b84a",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f02357c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Tesla Inc is going to acquire twitter for $45 billion.\")\n",
    "\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54d6f783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Captain America ate 100$ of samosa. Then he said I can do \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    this all day\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5151abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc2, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4194c",
   "metadata": {},
   "source": [
    "## Trained processing pipeline in French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be187f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa657e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "# nlp = fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b83d9f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"C'\", 'PRON'), ('est', 'AUX'), ('une', 'DET'), ('phrase', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"C'est une phrase.\")\n",
    "print([(w.text, w.pos_) for w in doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afd44a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Twitter  |  MISC  |  Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars.\")\n",
    "\n",
    "for ent in doc4.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3f172f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  PROPN  |  Tesla\n",
      "Inc  |  PROPN  |  Inc\n",
      "va  |  VERB  |  aller\n",
      "racheter  |  VERB  |  racheter\n",
      "Twitter  |  VERB  |  twitter\n",
      "pour  |  ADP  |  pour\n",
      "$  |  NOUN  |  dollar\n",
      "45  |  NUM  |  45\n",
      "milliards  |  NOUN  |  milliard\n",
      "de  |  ADP  |  de\n",
      "dollars  |  NOUN  |  dollar\n",
      ".  |  PUNCT  |  .\n"
     ]
    }
   ],
   "source": [
    "for token in doc4:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a1da0",
   "metadata": {},
   "source": [
    "### Adding a component to a blank pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed5cf0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3fa56fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Tesla Inc is going to acquire twitter for $45 billion.\")\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18320c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44935a50",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae6c5f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  #creating an object and loading the pre-trained model for \"English\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d78fb7",
   "metadata": {},
   "source": [
    "**Question: 1**\n",
    "\n",
    "- Get all the proper nouns from a given text in a list and also count how many of them.\n",
    "- **Proper Noun** means a noun that names a particular person, place, or thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a611ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Ravi and Raju are the best friends from school days.They wanted to go for a world tour and \n",
    "visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.\n",
    "They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!\n",
    "'''\n",
    "\n",
    "# https://spacy.io/usage/linguistic-features\n",
    "\n",
    "new_doc1 = nlp(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "980154c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proper Nouns:  [Raju, Paris, London, Dubai, Rome, Mohan, Hyderabad]\n",
      "Count:  7\n"
     ]
    }
   ],
   "source": [
    "all_proper_nouns = []  \n",
    "\n",
    "for token in new_doc1:\n",
    "    if token.pos_ == \"PROPN\":        #checking the whether token belongs to parts of speech \"PROPN\" [Proper Noun]\n",
    "        all_proper_nouns.append(token)\n",
    "        \n",
    "\n",
    "print(\"Proper Nouns: \", all_proper_nouns)\n",
    "print(\"Count: \", len(all_proper_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a631a98",
   "metadata": {},
   "source": [
    "**Question: 2**\n",
    "\n",
    "- Get all companies names from a given text and also the count of them.\n",
    "- **Hint**: Use the spacy **ner** functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70ff5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel'''\n",
    "\n",
    "new_doc2 = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "245918ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Names:  [Tesla, Walmart, Amazon, Microsoft, Google, Infosys, Reliance, HDFC Bank, Hindustan Unilever, Bharti]\n",
      "Count:  10\n"
     ]
    }
   ],
   "source": [
    "#list for storing the company names\n",
    "all_company_names = []\n",
    "\n",
    "for ent in new_doc2.ents:\n",
    "    if ent.label_ == 'ORG':     #checking the whether token belongs to entity \"ORG\" [Organisation]\n",
    "        all_company_names.append(ent)\n",
    "\n",
    "\n",
    "print(\"Company Names: \", all_company_names)\n",
    "print(\"Count: \", len(all_company_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa9454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
