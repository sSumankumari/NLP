{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVrcHNZpQLUz"
   },
   "source": [
    "### **TF-IDF: Exercises**\n",
    " \n",
    "- Humans ðŸ‘¦ show different emotions/feelings based on the situations and communicate them through facial expressions or in form of words.\n",
    " \n",
    "- In Social Media like Twitter and Instagram, many people express their views through comments about a particular event/scenario and these comments may address the feelings like sadness, happiness, joy, sarcasm, fear, and many other.\n",
    " \n",
    "- For a given comment/text, we are going to use classical NLP techniques and classify under which emotion that particular comment belongs!\n",
    " \n",
    "- We are going to use techniques like Bag of grams, n-grams, TF-IDF, etc. for text representation and apply different classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wU5KDovsV9ez"
   },
   "source": [
    "### **About Data: Emotion Detection**\n",
    "\n",
    "Credits: https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp\n",
    "\n",
    "\n",
    "- This data consists of two columns.\n",
    "        - Comment\n",
    "        - Emotion\n",
    "- Comment are the statements or messages regarding to a particular event/situation.\n",
    "\n",
    "- Emotion feature tells whether the given comment is fear ðŸ˜¨, Anger ðŸ˜¡, Joy ðŸ˜‚.\n",
    "\n",
    "- As there are only 3 classes, this problem comes under the **Multi-Class Classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "8ML2s0KWVXmv",
    "outputId": "4284d452-b3f4-4998-a2bf-9fd7aa273207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5937, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i seriously hate one subject to death but now ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im so full of life i feel appalled</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i sit here to write i start to dig out my feel...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ive been really angry with r and i feel like a...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i feel suspicious if there is no one outside l...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment Emotion\n",
       "0  i seriously hate one subject to death but now ...    fear\n",
       "1                 im so full of life i feel appalled   anger\n",
       "2  i sit here to write i start to dig out my feel...    fear\n",
       "3  ive been really angry with r and i feel like a...     joy\n",
       "4  i feel suspicious if there is no one outside l...    fear"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Emotion_classify_Data.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "joLuZmFpT-fY",
    "outputId": "77d48614-f6b3-4227-e90e-dc8ddb17381b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anger    2000\n",
       "joy      2000\n",
       "fear     1937\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vfy9KpUHW3Mg"
   },
   "source": [
    "- From the above, we can see that almost the Emotions(classes) occured equal number of times and balanced. There is **no problem of class imbalance** and hence no need to apply any balancing techniques like undersampling, oversampling etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "qPxiqT_TT-hx",
    "outputId": "d58985b6-c81c-48b5-de9b-74735bb6b133"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Emotion_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i seriously hate one subject to death but now ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im so full of life i feel appalled</td>\n",
       "      <td>anger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i sit here to write i start to dig out my feel...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ive been really angry with r and i feel like a...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i feel suspicious if there is no one outside l...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment Emotion  Emotion_num\n",
       "0  i seriously hate one subject to death but now ...    fear            1\n",
       "1                 im so full of life i feel appalled   anger            2\n",
       "2  i sit here to write i start to dig out my feel...    fear            1\n",
       "3  ive been really angry with r and i feel like a...     joy            0\n",
       "4  i feel suspicious if there is no one outside l...    fear            1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion_num'] = df['Emotion'].map({'joy' : 0, 'fear': 1, 'anger': 2})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PE-c0zbDXTEm"
   },
   "source": [
    "### **Modelling without Pre-processing Text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "NjJqi7UBT-nr"
   },
   "outputs": [],
   "source": [
    "#Do the 'train-test' splitting with test size of 20%\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.Comment, \n",
    "    df.Emotion_num, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022,\n",
    "    stratify=df.Emotion_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lAD0iqGcdCn",
    "outputId": "4efb3c3c-0ad4-4501-d815-35a902095848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (4749,)\n",
      "Shape of X_test:  (1188,)\n"
     ]
    }
   ],
   "source": [
    "#print the shapes\n",
    "\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t57hw7gOVXuW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6h8ZZLxZd79"
   },
   "source": [
    "\n",
    "**Attempt 1** :\n",
    "\n",
    "1. using the sklearn pipeline module create a classification pipeline to classify the data.\n",
    "\n",
    "**Note:**\n",
    "- using CountVectorizer with only trigrams.\n",
    "- use **RandomForest** as the classifier.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGg2iXv6g40l",
    "outputId": "3f895ca5-7606-4220-b9a2-7c26d9160a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.26      0.36       400\n",
      "           1       0.36      0.81      0.50       388\n",
      "           2       0.54      0.20      0.30       400\n",
      "\n",
      "    accuracy                           0.42      1188\n",
      "   macro avg       0.50      0.42      0.38      1188\n",
      "weighted avg       0.50      0.42      0.38      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_tri_grams', CountVectorizer(ngram_range = (3, 3))),                       #using the ngram_range parameter \n",
    "    ('random_forest', (RandomForestClassifier()))         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I08-kc_JYCNL"
   },
   "source": [
    "\n",
    "**Attempt 2** :\n",
    "\n",
    "1. using the sklearn pipeline module create a classification pipeline to classify the data.\n",
    "\n",
    "**Note:**\n",
    "- using CountVectorizer with both unigram and bigrams.\n",
    "- use **Multinomial Naive Bayes** as the classifier.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zetSmBrXmjM",
    "outputId": "b372f53c-8cbc-496f-ef4d-9fecff044230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87       400\n",
      "           1       0.87      0.83      0.85       388\n",
      "           2       0.83      0.88      0.85       400\n",
      "\n",
      "    accuracy                           0.86      1188\n",
      "   macro avg       0.86      0.86      0.86      1188\n",
      "weighted avg       0.86      0.86      0.86      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_bigrams', CountVectorizer(ngram_range = (1, 2))),        #using the ngram_range parameter \n",
    "     ('Multi NB', MultinomialNB())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Wde4r_-YwU-"
   },
   "source": [
    "\n",
    "**Attempt 3** :\n",
    "\n",
    "1. using the sklearn pipeline module create a classification pipeline to classify the data.\n",
    "\n",
    "**Note:**\n",
    "- using CountVectorizer with both unigram and bigrams.\n",
    "- use **RandomForest** as the classifier.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0dG2tc0X7SK",
    "outputId": "f6eddad9-a55b-4fde-eea2-2cbc002f6d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       400\n",
      "           1       0.93      0.89      0.91       388\n",
      "           2       0.94      0.86      0.90       400\n",
      "\n",
      "    accuracy                           0.90      1188\n",
      "   macro avg       0.91      0.90      0.90      1188\n",
      "weighted avg       0.91      0.90      0.90      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_bi_grams', CountVectorizer(ngram_range = (1, 2))),                       #using the ngram_range parameter \n",
    "    ('random_forest', (RandomForestClassifier()))         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmrXmL_3Z2y6"
   },
   "source": [
    "\n",
    "**Attempt 4** :\n",
    "\n",
    "1. using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "**Note:**\n",
    "- using **TF-IDF vectorizer** for pre-processing the text.\n",
    "- use **RandomForest** as the classifier.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djsDsThaaCSO",
    "outputId": "b4514ab2-f6ad-45e1-b91b-e88393e975e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       400\n",
      "           1       0.90      0.90      0.90       388\n",
      "           2       0.94      0.86      0.90       400\n",
      "\n",
      "    accuracy                           0.90      1188\n",
      "   macro avg       0.90      0.90      0.90      1188\n",
      "weighted avg       0.90      0.90      0.90      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),        \n",
    "     ('Random Forest', RandomForestClassifier())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ACq6pDkZ4sA"
   },
   "source": [
    "<h3>Use text pre-processing to remove stop words, punctuations and apply lemmatization </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "tj_xYgthX7UF"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load english language model and create nlp object from it\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "\n",
    "#use this utility function to get the preprocessed text data\n",
    "def preprocess(text):\n",
    "    # remove stop words and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(filtered_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "xqW1i19wX7Xq"
   },
   "outputs": [],
   "source": [
    "# create a new column \"preprocessed_comment\" and use the utility function above to get the clean data\n",
    "# this will take some time, please be patient\n",
    "df['preprocessed_comment'] = df['Comment'].apply(preprocess) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q24oRlMcai9l"
   },
   "source": [
    "**Build a model with pre processed text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ahdd2mgxX7dM"
   },
   "outputs": [],
   "source": [
    "#Do the 'train-test' splitting with test size of 20% with random state of 2022 and stratify sampling too\n",
    "#Note: Use the preprocessed_Comment\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.preprocessed_comment, \n",
    "    df.Emotion_num, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022,\n",
    "    stratify=df.Emotion_num\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqonfpeYasOE"
   },
   "source": [
    "**Let's check the scores with our best model till now**\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1wYgFs3auLQ"
   },
   "source": [
    "**Attempt1** :\n",
    "\n",
    "1. using the sklearn pipeline module create a classification pipeline to classify the data.\n",
    "\n",
    "**Note:**\n",
    "- using CountVectorizer with both unigrams and bigrams.\n",
    "- use **RandomForest** as the classifier.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Khtu32z1XmmE",
    "outputId": "b5f67051-d0e3-4e2e-e342-8297938090ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       400\n",
      "           1       0.95      0.91      0.93       388\n",
      "           2       0.92      0.94      0.93       400\n",
      "\n",
      "    accuracy                           0.93      1188\n",
      "   macro avg       0.93      0.93      0.93      1188\n",
      "weighted avg       0.93      0.93      0.93      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_bi_grams', CountVectorizer(ngram_range = (1, 2))),                       #using the ngram_range parameter \n",
    "    ('random_forest', (RandomForestClassifier()))         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9GZPaQbbJbx"
   },
   "source": [
    "\n",
    "**Attempt 2** :\n",
    "\n",
    "1. using the sklearn pipeline module create a classification pipeline to classify the data.\n",
    "\n",
    "**Note:**\n",
    "- using **TF-IDF vectorizer** for pre-processing the text.\n",
    "- use **RandomForest** as the classifier.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2y1Cy4Bauxu",
    "outputId": "ac6bf255-a218-400c-c4a9-790f4a89dfb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93       400\n",
      "           1       0.94      0.93      0.93       388\n",
      "           2       0.94      0.90      0.92       400\n",
      "\n",
      "    accuracy                           0.93      1188\n",
      "   macro avg       0.93      0.93      0.93      1188\n",
      "weighted avg       0.93      0.93      0.93      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),        #using the ngram_range parameter \n",
    "     ('Random Forest', RandomForestClassifier())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta2cWBUkfKel"
   },
   "source": [
    "## **Final Observations**\n",
    "\n",
    "- As part of this exercise we have trained the data with algorithms like **Multinomial Naive Bayes** and **Random Forest** which are most used and provide good results for text related problems.\n",
    "\n",
    "- As Machine learning algorithms do not work on text data directly, we need to convert them into numeric vectors and feed that into models while training. For this purpose, we have used Bag of words(unigrams, bigrams, n-grams) and TF-IDF text representation techniques.\n",
    "\n",
    "\n",
    "**Key Findings**\n",
    "\n",
    "- As the n_gram range keeps increasing, there's drastic fall of improvement in performance metrics.\n",
    "\n",
    "- There's seen a significant improvement in results before pre-processing and after pre-processing the data.\n",
    "\n",
    "- TF-IDF and Bag of words both performed equally well in performance metrics like Recall and F1-score.\n",
    "\n",
    "- Random Forest performed quite well when compared to Multinomial Naive Bayes.\n",
    "\n",
    "**Machine Learning is like a trial and error scientific method, where we keep trying all the possible algorithms we have and select the one which gives good results and satisfies the requirements like latency, interpretability, etc.**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tf_idf_exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
